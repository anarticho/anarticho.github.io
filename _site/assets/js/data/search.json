[
  
  {
    "title": "Spegel - Stateless Local OCI Mirror",
    "url": "/posts/Spegel-stateless-local-OCI-mirror/",
    "categories": "OCI, Mirror, Spegel",
    "tags": "OCI, Mirror, Spegel",
    "date": "2024-11-04 00:00:00 +0200",
    "content": "Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.  What is Spegel?     Stateless: Spegel doesn’t store image manifests or layers itself; it relies on containerd’s store on the node.   Local: It operates within your local K8s cluster.   OCI Registry: It’s compliant with the OCI distribution specification.   Mirror: It mirrors images from a remote registry to your local cluster. (Fun fact: “Spegel” means mirror in Swedish.)   Why Use Spegel?     Reduce Bandwidth Usage to remote registries.   Faster Image Pulls if images are cached within the K8s cluster.   Fault Tolerance if the remote registry is unavailable.   How Spegel Works  This blog focuses on visual diagrams that illustrate Spegel’s inner workings, making it easier to map component connections and debug issues.  Spegel Overview  Nodes in the cluster that need an image will first check if it’s available locally.     If available, they pull it directly from other nodes using the P2P network.   If not available, the system falls back to the remote registry.     Spegel: Visual Architecture Guide  Let’s dive into Spegel’s architecture through a series of diagrams:  1. High-Level Cluster Architecture  This diagram shows how Spegel pods form a P2P network within the cluster. Each Spegel pod interacts with containerd and, if necessary, falls back to the external registry.  Cluster Architecture  2. Pod Component Architecture  Displays the components within a Spegel pod, including registry services, P2P components, and state management.  Pod Component Architecture  3. Image Pull Flow  This sequence shows how an image pull request is handled, covering both peer pulls and fallback to external registry.  Image Pull Flow  4. P2P Network Formation  Illustrates how nodes discover each other and form the P2P network through leader election and peer sharing.  P2P Network Formation  5. State Management and Content Advertisement  Depicts how content availability is maintained and advertised across the P2P network.  State Management and Content Advertisement  6. Content Resolution Process  Content Resolution Process  7. Data Flow Paths  Describes content and control flow within the system, including peer transfers and fallback.  Data Flow Paths  8. Failure Handling  Demonstrates failure handling scenarios within the system.  Failure Handling  9. Metrics Collection  Shows how metrics are collected and organized across the system components.  Metrics Collection  Conclusion  I hope I gave a visual understanding of the Spegel project and its architecture. If you got intrigued, next step would be to dive into the source code at Spegel GitHub."
  },
  
  {
    "title": "How to Efficiently Serve an LLM?",
    "url": "/posts/How-to-Efficiently-serve-an-llm/",
    "categories": "LLM, Inference, Optimization, Serving",
    "tags": "LLM, inference, optimization, serving",
    "date": "2024-08-05 05:30:00 +0300",
    "content": "How to Efficiently Serve an LLM  LLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new records on various benchmarks and now often match or exceed human performance in multiple tasks GPT-4 Technical Report. Consequently, many companies are eager to deploy them in production. However, due to the unprecedented size of LLMs, there are significant challenges in serving them, such as slow token generation (tokens/second), memory limits for loading model parameters, KV cache (explained later), compute limits, and more. In this article, we will cover several recent ideas to help set up a robust LLM serving system.  LLM Serving Overview  LLM inference Steps     Multiple users send requests to the LLM Server through HTTPs/gRPC.   The LLM Server receives the requests and schedules them based on QoE definition.            QoE (Quality of Experience): Defined by two metrics:                    TTFT (Time to First Token): The time it takes for a user to receive the first response token.           TDS (Token Delivery Speed): The rate at which the user receives tokens, which should be uniform and above the reader’s reading speed for a positive user experience.                           QoE Aware LLM Serving      After scheduling, the LLM Inference process is divided into two phases:                     Prefill phase: The LLM processes the input tokens in parallel and generates the output activations known as the “KV Cache”. This step is highly efficient at utilizing the GPU’s parallel processing capabilities, making input tokens generally much cheaper than output tokens (as seen in the GPT-4o pricing chart). This phase produces the first output token and is typically compute-bound.                        GPT-4o Pricing                       Decode phase: The LLM starts autoregressively generating output tokens one at a time. This phase is slower in terms of inference and is where optimizations are necessary. Output tokens at each step are concatenated with the previous tokens’ KV cache to generate the next token.                        KV Cache Explanation &amp; Reuse                  Optimizations  Many experts are innovating the inference stack, and multiple startups are competing to reduce costs to attract more customers.     LLAMA 405 Pricing by Different Providers   Here are some interesting optimizations shared recently in research:     Batching:            Instead of serving one request at a time and wasting compute resources (since the decode phase has low arithmetic intensity and is memory-bound), we can amortize the cost of retrieving weights and KV cache from memory by serving multiple requests simultaneously.       Continuous Batching           Model Quantization (FP8/INT8):            Decreasing the precision of model weights and/or activations (AWQ/GPTQ) frees up more GPU VRAM, which allows for serving larger batches of requests.       Model Quantization           Paged Attention:            The core idea behind vLLM, the most popular open-source serving engine, is to avoid memory fragmentation that occurs due to preserving the max context length for every request by using paging (borrowed from OS paging) to manage memory efficiently.        Paged Attention in vLLM           Prefill Chunking / Stale-free Batching:            Proposed by the Sarathi-Serve paper, dividing the prefill context into smaller chunks allows merging the prefill and decode phases of different requests in the same batch.        Prefill Decode Prioritizing           Prefill/Decode Disaggregation:            In constrast to the previous idea, this paper Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving proposes separating the prefill and decode phases and transferring KVCache through a specialized design.        KVCache Transfer in Disaggregated Architecture           KVCache Compression:            As proposed by CacheGen, compressing the KVCache to speed up network transfer. This approach is beneficial for use cases with large context lengths (i.e., content summarization) which are over 16k input tokens to justify the encoding/decoding CPU overhead.        KV Cache Compression           Speculative Decoding:            Using extra smaller model(s) that generate tokens fast and in parallel. Selecting the output that matches the original model can speed up inference for simple use cases. Note that as the request batch size increases, the speed-up of speculative decoding diminishes.       Speculative Decoding           Radix Attention (Prefix Caching):            This is the idea behind SGLang (SGLang: Efficient Execution of Structured Language Model Programs), which involves creating a data structure similar to a Prefix tree (Trie) for the KVCache to help reuse KVCache without recomputation. This only works for some use cases, like those shown in the image below:       KV Cache Sharing Examples           Early Rejection:            Predicting if a request can be served once received to avoid wasted resources (i.e., the server successfully computed the prefill part but failed at the decode phase due to memory limitations) will help improve server resource utilization and prevent downtime.       Early Rejection based on Prediction           Conclusion  Efficiently serving large language models is essential for businesses to reduce costs and increase generation speed (tokens/second). This efficiency opens the door for more use cases for LLMs. With the ideas presented here, you can optimize your LLM inference stack to achieve these goals and more!  References     Improving LLM Inference with Prefill Chunking / Stale-free batching (USENIX)   Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving   KVCache Compression and Streaming for Faster LLM Serving (arXiv)   Dynamic Memory Management for LLMs: vAttention (arXiv)   Enhancing Quality-of-Experience in LLM-Based Services (arXiv)   Prefix Caching for Efficient LLM Inference (arXiv)   Mastering LLM Techniques: Inference Optimization (NVIDIA Technical Blog)   Token Probability Distribution (Hugging Face)   Welcome to vLLM! — vLLM Documentation   Serving Large Language Models: Technologies and Choices (run.ai)   Efficient Large Language Model Serving (arXiv)     Related: If you’re interested in the training side of LLMs, check out my post on What Infrastructure does it take to train a 405B Llama3-like model?"
  },
  
  {
    "title": "What Infrastructure does it take to train a 405B Llama3-like model?",
    "url": "/posts/What-Infra-does-it-take-to-train-llama405b/",
    "categories": "LLM, Infrastructure, GPU, Distributed Training",
    "tags": "LLM, infrastructure, GPU, distributed training",
    "date": "2024-07-28 00:00:00 +0300",
    "content": "Intro  Setting up the infrastructure for training the latest frontier models is not an easy feat; only a few companies have the scale to do it (Meta, Microsoft, Google, …). ML training has escalated from requiring up to 512 GPUs to needing 16k H100 GPUs to train Meta’s latest Llama3-405B model. This posed a huge challenge for infrastructure setup, necessitating significant innovation to handle this sheer number of GPUs working in tandem, as LLM distributed training jobs require synchronous communication and gang scheduling.  Understanding the underlying infrastructure used to train the latest LLMs is essential for ML scientists to maximize the MFU (Model FLOPs Utilization), especially as infrastructure costs rise. AI labs are currently racing to build the first 100K H100 cluster that would cost an estimated $4 billion. With that in mind, here’s an overview of the components required for building the infrastructure for the latest &amp; greatest LLMs.   Meta’s 24k H100 cluster design with a 7:1 oversubscription ratio  Network Topology  The first and most important step is designing the networking flow of gradients across the huge number of GPUs. As aforementioned, distributed training requires synchronous communication methods like All-reduce, all-gather, and broadcast to combine and share gradients. As model sizes increase (reportedly 1.8 trillion parameters for GPT-4), different parallelism techniques are required (Tensor, Context, Pipeline, Data) known as 4D parallelism that necessitate efficent design and communication.  In the ideal scenario, a GPU can communicate with any other GPU at full bandwidth (400 Gbps) using the latest Infiniband connection speed. However, achieving this for clusters of 100k GPUs would require a vast number of switches and transceivers to handle the communication traffic, making it cost prohibitive. To mitigate this, network architects trade-off by oversubscribing the aggregation top layer (as shown in the figure of Meta’s 24K cluster design with a 7:1 ratio) to reduce the overall cost.  Deciding the communication patterns to be network-aware is essential to efficiently utilize the hardware and avoid stragglers (slower-performing nodes in a distributed system) that could slow down the entire cluster. For example, Meta forked Nvidia’s NCCL library to optimize the communication patterns to fit their cluster design.  Storage  Training LLMs is memory-bound. While compute capabilities have rapidly evolved from different versions of GPUs (A100 → H100), the maximum memory capacity per GPU has increased, though not as dramatically as compute power. For example, A100 GPUs typically have up to 80 GB of HBM2e memory, whereas H100 GPUs can have up to 80 GB or more of HBM3 memory. More memory is essential for storing model weights, activations, and optimizer states (with Adam being the most popular optimizer, storing 3x parameters: one for the parameters themselves, one for the first moment (mean of gradients), and one for the second moment (variance of gradients)). With the rumored size of GPT-4 (1.8 trillion parameters), a total of 10.8 terabytes of memory would be required for training.  Additionally, memory is required for checkpointing (saving model weights frequently) to recover in case of failure or to choose the best-performing version of the model (if the model starts overfitting the data with more training). there are two ways to checkpoint:     Traditional way: offloading to CPU memory and then to persistent storage (adds delay but is simpler to do).   Recent way: using spare GPUs’ HBM to just RDMA copy the current model state for checkpointing; fast but require extra GPUs.   Storing datasets → 15.6 trillion tokens for Llama-3 required building 240 PB Storage for training, and fast data read speeds are needed to avoid wasting GPU cycles.  To ensure efficient training, the following aspects of storage must be considered:     High I/O Throughput: Ensuring fast data read and write speeds to avoid GPU downtime.   Scalability and Fault Tolerance: The storage system must scale with the growing dataset size and ensure data redundancy to protect against hardware failures.   Integration with Compute: Seamless integration with the compute infrastructure to allow high-speed data transfer between storage and GPUs.   Intermediate and Result Storage: Handling storage for intermediate results, logs, and multiple versions of model checkpoints efficiently.   Compute  Nvidia is the leader in compute with an estimated market share of 80% to 90% and becoming the most valuable company in the world. H100s are currently in mass production and were used in training Llama3-405B, with most AI labs competing to build using H100s due to their leading performance and AI-friendly software stack (CUDA &amp; cuDNN). AMD is trying to gain a share in this market with their MI250, and cloud providers are starting to build their own chips. Google’s TPU chips, used to train the Gemini family of models, are notable, but it doesn’t seem the market will change significantly in the foreseeable future.                 Feature       Nvidia H100       AMD MI250       Google TPU v4                       Architecture       Hopper       CDNA 2       TPU                 Memory (HBM)       80 GB       128 GB       128 GB                 Memory Bandwidth       3.0 TB/s       3.2 TB/s       2.7 TB/s                 Peak FP16 Throughput       198 TFLOPS       383 TFLOPS       275 TFLOPS                 NVLink       900 GB/s       -       -           Fault Tolerance &amp; Health Checks  Ensuring fault tolerance and performing regular health checks are crucial for maintaining the stability and efficiency of a large-scale AI training infrastructure. Given the high failure rates of GPUs and other components, robust fault tolerance mechanisms are essential to maximize hardware utilization and minimize downtime.  Key Fault Tolerance Strategies     Spare Capacity:            Imbue Recommendation is to maintain 10-20% more spare GPUs than required which allows quick replacement of failed GPUs, ensuring the training run is not halted due to a single failure.           Health Checks:            Implement scripts to detect faulty hardware (GPUs, InfiniBand, host machines, etc…).           Network Reliability:            Networks can fail due to flapping, host machine failures, or power supply issues, you need to use redundant paths and automatic failover mechanisms to ensure continuous operation.           Golden Sets of Machines:            Maintain a set of machines that have been stress-tested and verified to be reliable by running stress tests that maximize hardware utilization to distinguish between great and med machines.           Checkpointing:            Use spare GPUs’ HBM to RDMA (Remote Direct Memory Access) copy the current model state, offering a faster but more costly alternative. This method leverages the high bandwidth memory (HBM) of GPUs and the fast RDMA transfer speeds, reducing the time required for checkpointing.           Proactive Checks &amp; Automated Recovery:            Regularly measure and log power consumption, temperature, and fan speed.       Develop scripts for automated recovery processes, such as switching to spare GPUs and rerouting network traffic.           To illustrate the common issues faced during large-scale training runs, here are the interruption categories Meta encountered during their 54-day training run for Llama3:   Meta’s interruptions during 54-day training run  Conclusion  Understanding the infrastructure needs for training the frontier models might seem like a niche skill that only a few engineers in AI labs need. This is only true until an ML scientist faces a hardware error, which is inevitable due to the high percentage of failures of current hardware. Knowledge of the underlying infrastructure can help navigate these challenges with ease. Moreover, building software for training that fully utilizes the strengths and avoids the weaknesses in the infrastructure cluster will save a lot of money and might just be the differentiator between you and your competitors in this fast-paced space.  References     100k H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing (semianalysis.com)   The LLaMA 3 Herd of Models (Meta AI)   Building Meta’s GenAI Infrastructure (Meta Engineering)   Training Large Language Models at Scale (Meta Engineering)   70B Infrastructure (Imbue)     Related: Once you’ve trained a model, you need to serve it efficiently. Check out my post on How to Efficiently Serve an LLM?"
  },
  
  {
    "title": "The Tech Behind TikTok's Addictive Recommendation System",
    "url": "/posts/the-tech-behind-tiktoks-addictive-recommendation-system/",
    "categories": "TikTok, Recommendation Systems, Kafka, Flink",
    "tags": "tiktok, recommendation systems, kafka, flink",
    "date": "2023-12-04 21:30:00 +0200",
    "content": "Intro  I’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited details on how it works. So, I googled and found two papers “Monolith” and “IPS,” released by ByteDance, TikTok’s parent company, and here is the process as explained by them.  Recommendation features  There are features that influence the recommendation system, which are:  User Features:     User interactions, such as the videos you like, share, comment on, watch in full or skip, accounts you follow, accounts that follow you, and when you create content.   User information, such as your device and account settings, language preference, country, time zone and day, and device type.   Content Features:     Video information, such as captions, sounds, hashtags, number of video views, and the country in which the video was published.   These elements are merged to create detailed user profiles and content embeddings. These profiles and embeddings are then used to tailor content suggestions using a mix of collaborative and content-based approaches.  Model architecture  Embeddings are input to a Deep Factorization Machine, which leverages a deep neural network to capture non-linear and complex patterns from the embeddings, and Factorization Machines to capture linear correlations between different features, which is essential in understanding simple yet effective user-item interactions.   Model Architecture  Real-Time Online Training  TikTok has to update its recommendation system as fast as possible to account for the non-stationary nature of user data, known as “Concept Drift.” So, there has to be a mechanism that updates the model parameters in real-time.  “Monolith” framework uses a streaming engine that can be used in both batch and online training in a unified design. They use a Kafka queue to log actions of users (clicks, likes, comments, etc.) and another Kafka queue for features. And Flink as a streaming engine/job for joining them to create training examples.  During online training, sparse parameters are updated on a minute-level interval from the training PS (Parameter Synchronization) to the serving PS, which avoids heavy network transmissions or memory spikes.   Training Pipeline  Monolith uses TensorFlow’s distributed Worker-ParameterServer, where multiple machines work together to train a model. Workers are responsible for performing computations (gradients/parameter updates), while parameter servers are used for storing the current model state like weights and biases.    Managing Large and Dynamic User Data: Embedding Collisions  User data’s vast and dynamic nature poses a challenge, as it can lead to unwieldy embedding sizes and collisions (where different data points are mistakenly identified as identical).  To tackle this, “Monolith” employs “cuckoo hashing.” This method uses two hash tables with separate hash functions, allowing dynamic placement and repositioning of elements to avoid collisions.  Additionally, to prevent the embedding memory from expanding too rapidly, a probabilistic filter and an adjustable expiration period for embeddings are used, effectively managing the memory requirements.    Conclusion  TikTok’s recommendation system played a main role in its success and widespread use. I tried in this blog post to shed some light on the underlying technologies used, especially the online training, which helps them recommend real-time personalized content that keeps users staring at their phones for hours.  References:     How TikTok recommends content   Monolith: Real Time Recommendation System With Collisionless Embedding Table (arxiv.org)   *An Empirical Investigation of Personalization Factors on TikTok (arxiv.org)   cs.princeton.edu/courses/archive/spring21/cos598D/icde_2021_camera_ready.pdf"
  },
  
  {
    "title": "Do you really need a Vector Database?",
    "url": "/posts/do-you-really-need-a-vector-database/",
    "categories": "LLMs, GenAI, VectorDBs, Embeddings",
    "tags": "llms, genai, vectordbs, embeddings",
    "date": "2023-11-21 22:50:00 +0200",
    "content": "Do You Really Need a VectorDB?  Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector Databases have been posited as a de facto component in any LLM-powered application architecture. You might wonder why. The answer is seemingly straightforward: “LLMs have a limited context window, and to select the most relevant data, a VectorDB is necessary.” This seems logical at first, but let’s think more about it and consider the alternatives.   Spiderman meme of seeing vector databases everywhere  1. Why Vector Search over Keyword Search?  Although Vector Search is popular for its ability to deliver semantically similar results, something Keyword Search can’t do, it’s not without its drawbacks. Keyword search is more accurate, faster, and can scale with new data more effectively.  Keyword search excels in finding exact matches and can be easily updated with synonyms and taxonomies. In contrast, Vector search requires a vectorizer (embedding model) to process every query, which can incur additional costs and latency. It’s also susceptible to data drift, as the embedding model might become outdated, missing new terminology and relationships.  In essence, keyword search is more interpretable, cost-effective, and often provides better performance.  2. Why Vector Database over a Vector Index?  If you still think Vector Search is more suitable for your case, consider using a Vector Index like Facebook’s “FAISS,” which supports the most popular indexing algorithms like “HNSW” and various similarity measures such as “L2 distance”, “dot products”, and cosine similarity. It can also handle indices that don’t fit in-memory.  3. Why Vector Database over a General-Purpose Database?  Databases like PostgreSQL, with its pgvector extension, and Elasticsearch, with dense vector indexing, support vector storage, indexing, and similarity search capabilities, along with traditional database benefits like ACID compliance, point-in-time recovery, joins, etc.  But, Vector Databases definitely have their specialized use cases, especially for users managing billions of data embeddings. They benefit from architecture decisions that can improve performance over general-purpose databases, ensuring efficient storage of vectors and fast retrieval for similarity search calculations, while offering data safety. However, they aren’t the universal solution they’re often portrayed as.  Further Readings:     Beware Tunnel Vision in AI Retrieval - by Colin Harman   Do you actually need a vector database? - Ethan Rosenthal     Related: For more on LLM infrastructure, check out my posts on How to Efficiently Serve an LLM? and What Infrastructure does it take to train a 405B model?"
  },
  
  {
    "title": "Starting A Blog Hosted On Github Pages",
    "url": "/posts/how-to-build-website/",
    "categories": "Blogging, Tutorial",
    "tags": "github-pages, blog, personal blog, jekyll",
    "date": "2023-06-01 20:14:00 +0300",
    "content": "My first ever blog post  I’ve been thinking about starting a blog for a while now and I was procrastinating quite a bit :D. But, I finally did it and here I am writing my first ever blog post. Suprisingly enough, it will be about my experience setting up my blog and how you can do it too :D.   Scenario of a developer starting a blog  YouTube Video Walkthrough  Why Github Pages?  I’m a progammer. I’ve always wanted to have a personal website to showcase my projects and share my thoughts. I’ve looked into various blogging platforms like Wordpress, Medium, Substack, and Ghost. But, I chose Github Pages with Jekyll because I wanted to have:    Full control over my blog and I wanted to customize it to my taste.   A blog that is free and doesn’t require me to pay for hosting.   A blog that is simple, fast and easy to maintain doesn’t require me to spend hours to configure it.   Did I convince you? OK, now let’s break down the steps to setup your blogging site.  Step 1: Decide Your Theme  This step is to quickly browse through the various Jekyll themes available on various websites and pick one that fits your taste  Few sites where you can grab these templates:     https://jekyllthemes.io/   http://jekyllthemes.org/   https://jekyll-themes.com/   https://jamstackthemes.dev/ssg/jekyll/   I personally picked the Chirpy theme since it fits my expectations and it has a Dark theme :D.  Step 2: Activate Github Pages  Once you pick the Jekyll theme, it’s time to host it on Github Pages. The theme you picked usually comes with a set of instructions to configure and the instruction varies between different themes.  For Chirpy theme, the instructions are as follows:    Use the template to create your own repository.            Make sure to name it as &lt;your-gh-username&gt;.github.io       After doing this step Github actions will build and deploy your blog automatically to &lt;your-gh-username&gt;.github.io       But you don’t want just a template, you want to customized it to yourself. So, let’s move on to the next step.           Clone the repository you just created.   Install Ruby and Jekyll on your machine through the official guide.   Run bundle install to install the required gems.   Update the variables of _config.yml as needed. Some of them are typical options.            url is the address of your website       avatar is the profile picture in the sidebar       timezone is used to display the date and time of your posts       lang is the language of the site           Run bundle exec jekyll s to start the local server.    The empty template you should see  If you face any issues, you can refer to the Chripy theme’s Getting started guide.  Step 3: Setup Your Custom Root Domain  You need to visit one of the domain name registrar to buy a custom domain. There are multiple registrars to choose from:     GoDaddy   Google Domains   Name Cheap   … and many more   I personally chose GoDaddy since I had to pay ~10$ for 2-years plan  Configure Your Domain  After you purchase your domain, go into your domain management portal, click on manage DNS and add A type DNS records for github pages.                 Type       Data                       A       185.199.108.153                 A       185.199.109.153                 A       185.199.110.153                 A       185.199.111.153                 CNAME       gh-username.github.io           (These A type DNS records map your domain name to the Github’s IP address)  So far, my DNS record looks like this:   My GoDaddy’s DNS records  Configure Github Pages  Now that you have your domain’s DNS setup, Let’s head back to Github and configure your Github Pages to use your custom domain.     Go to your repository’s settings page.   Scroll down to the Pages section.   Under Custom domain enter your domain name and click Save.    My Github Pages Custom Domain page  Best Practice : Click on Enforce HTTPS to serve your blog via secure SSL connection. Your site will be configured with a free SSL certificate from Let’s Encrypt.  Bonus Tip: Test Your Site  If you see a 404 error or Domain Not Found error your DNS record might not be updated. Every time you update a DNS record, it takes few mins to several hours to propagate the WWW. So, give it sometime. To see if your domain is reachable, you could dig the DNS:  $ dig YOUR-DOMAIN.COM +noall +answer   Here is a reference from digging my website:  $ dig ahmedtremo.com +noall +answer ahmedtremo.com.    0    IN    A    185.199.111.153 ahmedtremo.com.    0    IN    A    185.199.109.153 ahmedtremo.com.    0    IN    A    185.199.110.153 ahmedtremo.com.    0    IN    A    185.199.108.153   Hope you found this article useful. If you have any questions, you can check my blog’s repo on Github or feel free to reach out to me on Twitter or LinkedIn."
  }
  
]

