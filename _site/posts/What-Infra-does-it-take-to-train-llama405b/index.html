<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="What Infrastructure does it take to train a 405B Llama3-like model?" />
<meta name="author" content="Tremo Ahmed" />
<meta property="og:locale" content="en" />
<meta name="description" content="A comprehensive overview of the infrastructure required to train 405B parameter LLMs, covering network topology, storage, compute, and fault tolerance." />
<meta property="og:description" content="A comprehensive overview of the infrastructure required to train 405B parameter LLMs, covering network topology, storage, compute, and fault tolerance." />
<link rel="canonical" href="http://localhost:4000/posts/What-Infra-does-it-take-to-train-llama405b/" />
<meta property="og:url" content="http://localhost:4000/posts/What-Infra-does-it-take-to-train-llama405b/" />
<meta property="og:site_name" content="Tremo Ahmed" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-28T00:00:00+03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="What Infrastructure does it take to train a 405B Llama3-like model?" />
<meta name="twitter:site" content="@tremooo" />
<meta name="twitter:creator" content="@tremooo" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Tremo Ahmed","url":"https://www.linkedin.com/in/tremo1/"},"dateModified":"2024-07-28T00:00:00+03:00","datePublished":"2024-07-28T00:00:00+03:00","description":"A comprehensive overview of the infrastructure required to train 405B parameter LLMs, covering network topology, storage, compute, and fault tolerance.","headline":"What Infrastructure does it take to train a 405B Llama3-like model?","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/What-Infra-does-it-take-to-train-llama405b/"},"url":"http://localhost:4000/posts/What-Infra-does-it-take-to-train-llama405b/"}</script>
<!-- End Jekyll SEO tag -->


  <title>What Infrastructure does it take to train a 405B Llama3-like model? | Tremo Ahmed
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="icon" type="image/png" href="/assets/img/favicons/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/svg+xml" href="/assets/img/favicons/favicon.svg">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">



  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.36.4/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.18/dayjs.min.js,npm/dayjs@1.11.18/locale/en.js,npm/dayjs@1.11.18/plugin/relativeTime.js,npm/dayjs@1.11.18/plugin/localizedFormat.js,npm/tocbot@4.36.4/dist/tocbot.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/personal/ahmedtremo.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">Tremo Ahmed</a>
    <p class="site-subtitle fst-italic mb-0">Research Engineer @ Meta | AI Infra</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
          
      

      
        <a
          href="https://www.linkedin.com/in/tremo1"
          aria-label="linkedin"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-linkedin"></i>
        </a>
      
    

      
        <a
          href="https://github.com/tremo1"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    

      
        <a
          href="https://twitter.com/tremooo"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    

      
        <a
          href="javascript:location.href = 'mailto:' + ['me','tremo.uk'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
          
        

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>What Infrastructure does it take to train a 405B Llama3-like model?</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>What Infrastructure does it take to train a 405B Llama3-like model?</h1>
    
      <p class="post-desc fw-light mb-4">A comprehensive overview of the infrastructure required to train 405B parameter LLMs, covering network topology, storage, compute, and fault tolerance.</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1722114000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jul 28, 2024
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              
                <a href="https://www.linkedin.com/in/tremo1/">Tremo Ahmed</a>
                
              
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="1252 words"
>
  <em>6 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">What Infrastructure does it take to train a 405B Llama3-like model?</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">What Infrastructure does it take to train a 405B Llama3-like model?</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <h2 id="intro"><span class="me-2">Intro</span><a href="#intro" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Setting up the infrastructure for training the latest frontier models is not an easy feat; only a few companies have the scale to do it (Meta, Microsoft, Google, …). ML training has escalated from requiring up to <a href="https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/">512 GPUs to needing 16k H100 GPUs</a> to train Meta’s latest Llama3-405B model. This posed a huge challenge for infrastructure setup, necessitating significant innovation to handle this sheer number of GPUs working in tandem, as LLM distributed training jobs require synchronous communication and gang scheduling.</p>

<p>Understanding the underlying infrastructure used to train the latest LLMs is essential for ML scientists to maximize the MFU (Model FLOPs Utilization), especially as infrastructure costs rise. AI labs are currently racing to build the first 100K H100 cluster that would cost an estimated <a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network">$4 billion</a>. With that in mind, here’s an overview of the components required for building the infrastructure for the latest &amp; greatest LLMs.</p>

<p><a href="/assets/img/posts/2024-07-27-What-Infra-does-it-take-to-train-llama405b/Infra%20Networking%20cluster.jpg" class="popup img-link shimmer"><img src="/assets/img/posts/2024-07-27-What-Infra-does-it-take-to-train-llama405b/Infra%20Networking%20cluster.jpg" alt="Meta's 24k Cluster" loading="lazy"></a>
<strong>Meta’s 24k H100 cluster design with a 7:1 oversubscription ratio</strong></p>

<h2 id="network-topology"><span class="me-2">Network Topology</span><a href="#network-topology" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>The first and most important step is designing the networking flow of gradients across the huge number of GPUs. As aforementioned, distributed training requires synchronous communication methods like All-reduce, all-gather, and broadcast to combine and share gradients. As model sizes increase (reportedly <a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network">1.8 trillion</a> parameters for GPT-4), different parallelism techniques are required (Tensor, Context, Pipeline, Data) known as 4D parallelism that necessitate efficent design and communication.</p>

<p>In the ideal scenario, a GPU can communicate with any other GPU at full bandwidth (400 Gbps) using the latest Infiniband connection speed. However, achieving this for clusters of 100k GPUs would require a vast number of switches and transceivers to handle the communication traffic, making it cost prohibitive. To mitigate this, network architects trade-off by <a href="https://networkengineering.stackexchange.com/a/60003">oversubscribing</a> the aggregation top layer (as shown in the figure of Meta’s 24K cluster design with a 7:1 ratio) to reduce the overall cost.</p>

<p>Deciding the communication patterns to be network-aware is essential to efficiently utilize the hardware and avoid stragglers (slower-performing nodes in a distributed system) that could slow down the entire cluster. For example, Meta forked Nvidia’s NCCL library to optimize the communication patterns to fit their cluster design.</p>

<h2 id="storage"><span class="me-2">Storage</span><a href="#storage" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Training LLMs is memory-bound. While compute capabilities have rapidly evolved from different versions of GPUs (A100 → H100), the maximum memory capacity per GPU has increased, though not as dramatically as compute power. For example, A100 GPUs typically have up to 80 GB of HBM2e memory, whereas H100 GPUs can have up to 80 GB or more of HBM3 memory. More memory is essential for storing model weights, activations, and optimizer states (with Adam being the most popular optimizer, storing 3x parameters: one for the parameters themselves, one for the first moment (mean of gradients), and one for the second moment (variance of gradients)). With the rumored size of GPT-4 (1.8 trillion parameters), a total of 10.8 terabytes of memory would be required for training.</p>

<p>Additionally, memory is required for checkpointing (saving model weights frequently) to recover in case of failure or to choose the best-performing version of the model (if the model starts overfitting the data with more training). there are two ways to checkpoint:</p>

<ul>
  <li><strong>Traditional way</strong>: offloading to CPU memory and then to persistent storage (adds delay but is simpler to do).</li>
  <li><strong>Recent way</strong>: using spare GPUs’ HBM to just RDMA copy the current model state for checkpointing; fast but require extra GPUs.</li>
</ul>

<p>Storing datasets → 15.6 trillion tokens for Llama-3 required building 240 PB Storage for training, and fast data read speeds are needed to avoid wasting GPU cycles.</p>

<p>To ensure efficient training, the following aspects of storage must be considered:</p>

<ol>
  <li><strong>High I/O Throughput</strong>: Ensuring fast data read and write speeds to avoid GPU downtime.</li>
  <li><strong>Scalability and Fault Tolerance</strong>: The storage system must scale with the growing dataset size and ensure data redundancy to protect against hardware failures.</li>
  <li><strong>Integration with Compute</strong>: Seamless integration with the compute infrastructure to allow high-speed data transfer between storage and GPUs.</li>
  <li><strong>Intermediate and Result Storage</strong>: Handling storage for intermediate results, logs, and multiple versions of model checkpoints efficiently.</li>
</ol>

<h2 id="compute"><span class="me-2">Compute</span><a href="#compute" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Nvidia is the leader in compute with an estimated market share of 80% to 90% and becoming the most valuable company in the world. H100s are currently in mass production and were used in training Llama3-405B, with most AI labs competing to build using H100s due to their leading performance and AI-friendly software stack (CUDA &amp; cuDNN). AMD is trying to gain a share in this market with their MI250, and cloud providers are starting to build their own chips. Google’s TPU chips, used to train the Gemini family of models, are notable, but it doesn’t seem the market will change significantly in the foreseeable future.</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Nvidia H100</th>
      <th>AMD MI250</th>
      <th>Google TPU v4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Architecture</td>
      <td>Hopper</td>
      <td>CDNA 2</td>
      <td>TPU</td>
    </tr>
    <tr>
      <td>Memory (HBM)</td>
      <td>80 GB</td>
      <td>128 GB</td>
      <td>128 GB</td>
    </tr>
    <tr>
      <td>Memory Bandwidth</td>
      <td>3.0 TB/s</td>
      <td>3.2 TB/s</td>
      <td>2.7 TB/s</td>
    </tr>
    <tr>
      <td>Peak FP16 Throughput</td>
      <td>198 TFLOPS</td>
      <td>383 TFLOPS</td>
      <td>275 TFLOPS</td>
    </tr>
    <tr>
      <td>NVLink</td>
      <td>900 GB/s</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
</table></div>

<h2 id="fault-tolerance--health-checks"><span class="me-2">Fault Tolerance &amp; Health Checks</span><a href="#fault-tolerance--health-checks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Ensuring fault tolerance and performing regular health checks are crucial for maintaining the stability and efficiency of a large-scale AI training infrastructure. Given the high failure rates of GPUs and other components, robust fault tolerance mechanisms are essential to maximize hardware utilization and minimize downtime.</p>

<h3 id="key-fault-tolerance-strategies"><span class="me-2">Key Fault Tolerance Strategies</span><a href="#key-fault-tolerance-strategies" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ol>
  <li><strong>Spare Capacity</strong>:
    <ul>
      <li><a href="https://imbue.com/research/70b-infrastructure/">Imbue Recommendation</a> is to maintain 10-20% more spare GPUs than required which allows quick replacement of failed GPUs, ensuring the training run is not halted due to a single failure.</li>
    </ul>
  </li>
  <li><strong>Health Checks</strong>:
    <ul>
      <li>Implement scripts to detect faulty hardware (GPUs, InfiniBand, host machines, etc…).</li>
    </ul>
  </li>
  <li><strong>Network Reliability</strong>:
    <ul>
      <li>Networks can fail due to flapping, host machine failures, or power supply issues, you need to use redundant paths and automatic failover mechanisms to ensure continuous operation.</li>
    </ul>
  </li>
  <li><strong>Golden Sets of Machines</strong>:
    <ul>
      <li>Maintain a set of machines that have been stress-tested and verified to be reliable by running stress tests that maximize hardware utilization to distinguish between great and med machines.</li>
    </ul>
  </li>
  <li><strong>Checkpointing</strong>:
    <ul>
      <li>Use spare GPUs’ HBM to RDMA (Remote Direct Memory Access) copy the current model state, offering a faster but more costly alternative. This method leverages the high bandwidth memory (HBM) of GPUs and the fast RDMA transfer speeds, reducing the time required for checkpointing.</li>
    </ul>
  </li>
  <li><strong>Proactive Checks &amp; Automated Recovery</strong>:
    <ul>
      <li>Regularly measure and log power consumption, temperature, and fan speed.</li>
      <li>Develop scripts for automated recovery processes, such as switching to spare GPUs and rerouting network traffic.</li>
    </ul>
  </li>
</ol>

<p>To illustrate the common issues faced during large-scale training runs, here are the interruption categories Meta encountered during their 54-day training run for Llama3:</p>

<p><a href="/assets/img/posts/2024-07-27-What-Infra-does-it-take-to-train-llama405b/Meta%20interruptions.png" class="popup img-link shimmer"><img src="/assets/img/posts/2024-07-27-What-Infra-does-it-take-to-train-llama405b/Meta%20interruptions.png" alt="Meta Interruptions" loading="lazy"></a>
<strong>Meta’s interruptions during 54-day training run</strong></p>

<h2 id="conclusion"><span class="me-2">Conclusion</span><a href="#conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Understanding the infrastructure needs for training the frontier models might seem like a niche skill that only a few engineers in AI labs need. This is only true until an ML scientist faces a hardware error, which is inevitable due to the high percentage of failures of current hardware. Knowledge of the underlying infrastructure can help navigate these challenges with ease. Moreover, building software for training that fully utilizes the strengths and avoids the weaknesses in the infrastructure cluster will save a lot of money and might just be the differentiator between you and your competitors in this fast-paced space.</p>

<h2 id="references"><span class="me-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<ol>
  <li><a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network">100k H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing (semianalysis.com)</a></li>
  <li><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">The LLaMA 3 Herd of Models (Meta AI)</a></li>
  <li><a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta’s GenAI Infrastructure (Meta Engineering)</a></li>
  <li><a href="https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/">Training Large Language Models at Scale (Meta Engineering)</a></li>
  <li><a href="https://imbue.com/research/70b-infrastructure/">70B Infrastructure (Imbue)</a></li>
</ol>

<hr />

<p><strong>Related:</strong> Once you’ve trained a model, you need to serve it efficiently. Check out my post on <a href="/posts/How-to-Efficiently-serve-an-llm/">How to Efficiently Serve an LLM?</a></p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/llm/">LLM</a>,
          <a href="/categories/infrastructure/">Infrastructure</a>,
          <a href="/categories/gpu/">GPU</a>,
          <a href="/categories/distributed-training/">Distributed Training</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/llm/"
            class="post-tag no-text-decoration"
          >LLM</a>
        
          <a
            href="/tags/infrastructure/"
            class="post-tag no-text-decoration"
          >infrastructure</a>
        
          <a
            href="/tags/gpu/"
            class="post-tag no-text-decoration"
          >GPU</a>
        
          <a
            href="/tags/distributed-training/"
            class="post-tag no-text-decoration"
          >distributed training</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FWhat-Infra-does-it-take-to-train-llama405b%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" aria-label="Linkedin">
        <i class="fa-fw fab fa-linkedin"></i>
      </a>
    

      

      <a href="https://twitter.com/intent/tweet?text=What%20Infrastructure%20does%20it%20take%20to%20train%20a%20405B%20Llama3-like%20model?%20-%20Tremo%20Ahmed&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FWhat-Infra-does-it-take-to-train-llama405b%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=What%20Infrastructure%20does%20it%20take%20to%20train%20a%20405B%20Llama3-like%20model?%20-%20Tremo%20Ahmed&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FWhat-Infra-does-it-take-to-train-llama405b%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FWhat-Infra-does-it-take-to-train-llama405b%2F&text=What%20Infrastructure%20does%20it%20take%20to%20train%20a%20405B%20Llama3-like%20model?%20-%20Tremo%20Ahmed" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Spegel-stateless-local-OCI-mirror/">Spegel - Stateless Local OCI Mirror</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/How-to-Efficiently-serve-an-llm/">How to Efficiently Serve an LLM?</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/What-Infra-does-it-take-to-train-llama405b/">What Infrastructure does it take to train a 405B Llama3-like model?</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/the-tech-behind-tiktoks-addictive-recommendation-system/">The Tech Behind TikTok's Addictive Recommendation System</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/do-you-really-need-a-vector-database/">Do you really need a Vector Database?</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/llm/">LLM</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/blog/">blog</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed training</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/flink/">flink</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/genai/">genai</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/github-pages/">github-pages</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/gpu/">GPU</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/inference/">inference</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/infrastructure/">infrastructure</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    
  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/How-to-Efficiently-serve-an-llm/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1722825000"
  data-df="ll"
  
>
  Aug  5, 2024
</time>

              <h4 class="pt-0 my-2">How to Efficiently Serve an LLM?</h4>
              <div class="text-muted">
                <p>Exploring LLM serving optimizations including batching, quantization, paged attention, speculative decoding, and KV cache techniques.</p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/the-tech-behind-tiktoks-addictive-recommendation-system/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>The Tech Behind TikTok's Addictive Recommendation System</p>
    </a>
  

  
    <a
      href="/posts/How-to-Efficiently-serve-an-llm/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>How to Efficiently Serve an LLM?</p>
    </a>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2026</time>

    
      <a href="https://twitter.com/tremooo">Tremo Ahmed</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.4.1"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/llm/">LLM</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/blog/">blog</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed training</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/flink/">flink</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/genai/">genai</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/github-pages/">github-pages</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/gpu/">GPU</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/inference/">inference</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/infrastructure/">infrastructure</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->

  
  <!-- https://giscus.app/ -->
<script>
  (function () {
    const themeMapper = Theme.getThemeMapper('light', 'dark_dimmed');
    const initTheme = themeMapper[Theme.visualState];

    let lang = 'en';if (lang.length > 2 && !lang.startsWith('zh')) {
      lang = lang.slice(0, 2);
    }

    let giscusAttributes = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'tremo1/tremo1.github.io',
      'data-repo-id': 'R_kgDOJp3hDw',
      'data-category': 'General',
      'data-category-id': 'DIC_kwDOJp3hD84C007K',
      'data-mapping': 'pathname',
      'data-strict' : '0',
      'data-reactions-enabled': '1',
      'data-emit-metadata': '0',
      'data-theme': initTheme,
      'data-input-position': 'bottom',
      'data-lang': lang,
      'data-loading': 'lazy',
      crossorigin: 'anonymous',
      async: ''
    };

    let giscusNode = document.createElement('script');
    Object.entries(giscusAttributes).forEach(([key, value]) =>
      giscusNode.setAttribute(key, value)
    );

    const $footer = document.querySelector('footer');
    $footer.insertAdjacentElement("beforebegin", giscusNode);

    addEventListener('message', (event) => {
      if (event.source === window && event.data && event.data.id === Theme.ID) {
        const newTheme = themeMapper[Theme.visualState];

        const message = {
          setConfig: {
            theme: newTheme
          }
        };

        const giscus =
          document.getElementsByClassName('giscus-frame')[0].contentWindow;
        giscus.postMessage({ giscus: message }, 'https://giscus.app');
      }
    });
  })();
</script>



    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

